<!DOCTYPE html>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js">
</script>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta property="og:title" content="Watch and Match: Supercharging Imitation with Regularized Optimal Transport">
  <meta property="og:description" content="Watch and Match: Supercharging Imitation with Regularized Optimal Transport">
  <meta property="og:type" content="website">
  <meta property="og:site_name" content="Watch and Match: Supercharging Imitation with Regularized Optimal Transport">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Watch and Match: Supercharging Imitation with Regularized Optimal Transport">
  <meta name="twitter:description"
    content="-Regularized Optimal Transport (ROT), a new imitation learning algorithm that substantially improves sample efficiency for continuous control problems.">
  <meta name="twitter:image" content="./mfiles/arch bet.001.png" />
  <link rel="shortcut icon" href="img/favicon.png">
  <link rel="stylesheet" href="css/simple-grid.css">
  <title>Watch and Match: Supercharging Imitation with Regularized Optimal Transport</title>
</head>

<body>
  <div class="jumbotron">
    <div class="container">
      <div class="row">
        <div class="col-13 center">
          <h1>Watch and Match: Supercharging Imitation with Regularized Optimal Transport</h1>
        </div>
        <div class="col-3 hidden-sm"></div>
        <div class="col-2 center">
          <a style="text-decoration: none" href="" download>
            <h3 style="color: #F5A803">Paper</h3>
          </a>
        </div>
        <div class="col-2 center">
          <a style="text-decoration: none" href="" download>
            <h3 style="color: #F5A803">Code</h3>
          </a>
        </div>
        <div class="col-2 center">
          <a style="text-decoration: none" href="" download>
            <h3 style="color: #F5A803">Data</h3>
          </a>
        </div>
      </div>

      <!--Abstract-->
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Abstract</h2>
          <p>
            Imitation learning holds tremendous promise in learning policies efficiently for complex decision making problems. 
            Current state-of-the-art algorithms often use inverse reinforcement learning (IRL), where given a set of expert 
            demonstrations, an agent alternatively infers a reward function and the associated optimal policy. However, such IRL 
            approaches often require substantial online interactions for complex control problems. In this work, we present 
            Regularized Optimal Transport (ROT), a new imitation learning algorithm that builds on recent advances in optimal 
            transport based trajectory-matching. Our key technical insight is that adaptively combining trajectory-matching rewards 
            with behavior cloning can significantly accelerate imitation even without task-specific rewards. Our experiments on 20 
            visual control tasks across the DeepMind Control Suite, the OpenAI Robotics Suite, and the Meta-World Benchmark, 
            demonstrate an average of 8.7x faster imitation to reach 90% of expert performance compared to prior state-of-the-art 
            methods. On real-world manipulation, with just one demonstration and an hour of online training, ROT achieves an average 
            success rate of 89% across 12 tasks.
          </p>
        </div>
      </div>
    </div>
    <!--Videos-->
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Real-World Results</h2>
          <p>We provide evaluation rollouts of ROT on a set of 12 real-world manipulation tasks. With just one demonstration and one hour of
             online training, ROT achieved an average sucess rate of 89% across 12 tasks. This is significantly higher than behavior 
             cloning (30%) and adversarial IRL (13%) based approaches.
          </p>
        </div>
      </div>
    </div>
    <br><br>
    <div class="body-content">
      <div class="container">
        <div class="grid-display">
          <br><br>
          <div class="row">
            <div class="col-6">
              <video class="img" style="height: 600" controls muted autoplay loop>
                <source src="./mfiles/env/pushblock/1.1 Push Block-2.mp4" type="video/mp4">
              </video>
            </div>
            <div class="col-6">
              <video class="img" style="height: 600" controls muted autoplay loop>
                <source src="./mfiles/env/pushblock/Final BlockPush-2.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <br><br>
          <div class="row">
            <div class="col-6">
              <video class="img" style="height: 600" controls muted autoplay loop>
                <source src="./mfiles/env/kitchen/1st Kitchen.mp4" type="video/mp4">
              </video>
            </div>
            <div class="col-6 center">
              <video class="img" style="height: 600" controls muted autoplay loop>
                <source src="./mfiles/env/kitchen/2nd Kitchen.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <br><br>
          <div class="row">
            <div class="col-6">
              <video class="img" controls muted autoplay loop>
                <source src="./mfiles/env/carla/1st Traj Over.mp4" type="video/mp4">
              </video>
            </div>
            <div class="col-6">
              <video class="img" style="width: 100%" controls muted autoplay loop>
                <source src="./mfiles/env/carla/2nd Traj Over.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <div class="row">

            <div class="col-6 left">
              <video class="img" style="width: 100%" controls muted autoplay loop>
                <source src="./mfiles/env/carla/1st Traj Obs-2.mp4" type="video/mp4">
              </video>
            </div>
            <div class="col-6 left">
              <video class="img" style="width: 100%" controls muted autoplay loop>
                <source src="./mfiles/env/carla/2nd Traj Obs-2.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!--Image-->
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Method</h2>
          <p><a style="color: #00A2FF; font-weight: bold;">Regularized Optimal Transport (ROT)</a> is a new imitation learning 
            algorithm that adaptively combines offline behavior cloning with online trajectory-matching based rewards (top). 
            This enables signficantly faster imitation across a variety of simulated and real robotics tasks, while being 
            compatible with high-dimensional visual observation. On our xArm robot, ROT can learn visual policies with only 
            a single human demonstration and under an hour of online training.
        </div>
      </div>
    </div>
    <div class="row">
      <div class="center img">
        <img src="./mfiles/corl_intro_fig.jpeg" style="max-width:1500px;width:50%" frameborder="0"
          allowfullscreen></img>
      </div>
    </div>

    <!--Method-->
    <div class="container">
      <div class="row">
        <div class="col-12">
          <p>Our main findings can be summarized as:
          <ul style="font-size: 1.125rem;font-weight: 200;line-height: 1.8">
            <li>ROT outperforms prior state-of-the-art imitation methods, reaching 90% of expert performance
              8.7% faster than our strongest baselines on simulated visual control benchmarks.</li>
            <li>On real-world tasks, with a single human demonstration and an hour of training, ROT achieves an
              average success rate of 89% with randomized robot initialization and image observations. This is
              significantly higher than behavior cloning (30%) and adversarial IRL (13%) based approaches.</li>
            <li>ROT exceeds the performance of state-of-the-art RL trained with rewards, while coming close to
              methods that augment RL with demonstrations. Unlike standard RL methods, ROT does not require 
              hand-specification of the reward function.</li>
            <li>Ablation studies demonstrate the importance of every component in ROT, particularly the role
              that soft Q-filtering plays in stabilizing training and the need for OT-based rewards during online
              learning.</li>
          </ul>
          </p>
        </div>
      </div>
    </div>

    <!--Experiments-->
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Experiments</h2>
          <p>To demonstrate the effectiveness of ROT, we run extensive experiments on 20 simulated tasks across
            DM Control, OpenAI Robotics, and Meta-world, and 12 robotic manipulation tasks on an xArm. For DM Control, we 
            measure the average episode reward. For OpenAI Robotics, Meta-world and the real-world xArm tasks, we measure the number
            of successful trajectories. Evaluations are over 10 trajectories for the simulated tasks and 20 trajectories for the 
            real-world tasks. To reach 90% of expert performance, ROT is on average
          </p>
          <ul style="font-size: 1.125rem;font-weight: 200;line-height: 1.8">
            <li> 8.9Ã— faster on DeepMind Control tasks</li>
            <img class="center" src="./mfiles/appendix_pixel_results_dmc.png" style="width:100%"></img>
            <li>2.37Ã— faster on OpenAI Robotics tasks</li>
            <img class="center" src="./mfiles/appendix_pixel_results_fetch.png" style="width:100%"></img>
            <li>11.5Ã— faster on Meta-world tasks</li>
            <img class="center" src="./mfiles/appendix_pixel_results_metaworld.png" style="width:100%"></img>
          </ul>
          </p>
          <p>The performance of ROT on the real-world manipulation tasks is provided below.
          </p>
          <img class="center" src="./mfiles/robot_results.png" style="width:100%"></img>
          <br><br>
          <!-- <img class="center" src="./mfiles/robot_results.png" style="width:100%"></img> -->
        </div>
      </div>
    </div>

    <!--Future Work-->
    <div class="container" style="padding-bottom: 150px; padding-top: 20px">
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Limitations and Future Work</h2>
          <p>In this work, we propose Regularized Optimal Transport (ROT), a new imitation learning algorithm
            that alleviates the challenge of exploration and significantly improves sample efficiency by using a
            pretrained policy in conjunction with an adaptive regularization scheme for online finetuning. We
            demonstrate superior performance compared to prior work on a varied set of simulated environments
            as well as a set of real-world manipulation tasks with just one demonstration and an hour of online
            training. However, there are a few limitations in this work: (a) Since our OT-based approach aligns
            agents with demonstrations without task-specific rewards, it relies on the demonstrator being an
            â€˜expertâ€™. Extending ROT to suboptimal demonstrations would be an exciting future direction. (b)
            Performing BC pretraining and BC-based regularization requires access to expert actions, which may
            not be present in real-world demonstrations from humans. Recent work on using inverse models to
            infer actions given observational data could alleviate this challenge. We look forward to future
            work that extends ROT to these scenarios with suboptimal demonstrations or unavailability of expert
            actions.
          </p>
        </div>
      </div>
    </div>

  </div>
  <footer>
  </footer>
</body>

</html>